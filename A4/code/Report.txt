The folder contains:
1. RNN.py
2. RNN.ipynb: Helps view the training process better
3. logs: Hold the relevant model parameters

* Run 'make' command to download the model weights. Extract the downloaded zip archive, and then run the code.

To train, enter command: 'python2 RNN.py --train --hidden_unit n --model y'
	* y can be lstm or gru
	* the command trains the corresponding model with required hidden unit size
	e.g.: python2 RNN.py --train --hidden_unit 100 --model lstm

Note: If you have way too much time on your hands, and really want to train the model
using our code, we suggest training with learning rate 1e-3, 1e-4 and then 5e-5 for 10, 
15 and 20 epochs respectively.

To test, enter command: 'python2 RNN.py --test --hidden_unit n --model y'
	* y can, again, only be lstm or gru
	* it reloads the model weights from the relevant logbook and evaluates model
	e.g. python2 RNN.py --test --hidden_unit 100 --model lstm

We've already trained the model for n = 50, 100, 200, and achieved the following results:

Hidden layer size vs Test accuracy
LSTM:
	50: 83.38%
	100: 84.59%
	200: 85.95%

GRU:
	50: 81.00%
	100: 82.76%
	200: 84.57%
