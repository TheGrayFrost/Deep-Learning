{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "'''load your data here'''\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self):\n",
    "        DIR = '../data/'\n",
    "        pass\n",
    "    \n",
    "    # Returns images and labels corresponding for training and testing. Default mode is train. \n",
    "    # For retrieving test data pass mode as 'test' in function call.\n",
    "    def load_data(self, mode = 'train'):\n",
    "        label_filename = mode + '_labels'\n",
    "        image_filename = mode + '_images'\n",
    "        label_zip = '../data/' + label_filename + '.zip'\n",
    "        image_zip = '../data/' + image_filename + '.zip'\n",
    "        with ZipFile(label_zip, 'r') as lblzip:\n",
    "            labels = np.frombuffer(lblzip.read(label_filename), dtype=np.uint8, offset=8)\n",
    "        with ZipFile(image_zip, 'r') as imgzip:\n",
    "            images = np.frombuffer(imgzip.read(image_filename), dtype=np.uint8, offset=16).reshape(len(labels), 784)\n",
    "        return images, labels\n",
    "\n",
    "    def create_batches(self, img, lab, s = 10):\n",
    "        r = np.random.randint(img.shape[0], size = s)\n",
    "        return img[r], lab[r]\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    x[x < 0] = 0\n",
    "    return x\n",
    "\n",
    "class NN(object):\n",
    "    \n",
    "    # random initiation\n",
    "    def __init__(self, h, inp, op):\n",
    "        self.inp = inp\n",
    "        self.hn = h\n",
    "        self.op = op\n",
    "        self.W1 = np.random.normal(0, 0.01, (inp+1, h))\n",
    "        self.W2 = np.random.normal(0, 0.01, (h+1, op))\n",
    "        pass\n",
    "    \n",
    "    # forward pass\n",
    "    def forward(self, inp):\n",
    "        self.inter = []\n",
    "        self.inter.append(inp)\n",
    "        h = np.append(inp, np.ones((inp.shape[0], 1)), axis = 1)\n",
    "        self.inter.append(h)\n",
    "        \n",
    "        # hidden layer\n",
    "        r = ReLU(np.matmul(h, self.W1))\n",
    "        self.inter.append(r)\n",
    "        v = np.append(r, np.ones((r.shape[0], 1)), axis = 1)\n",
    "        self.inter.append(v)\n",
    "        \n",
    "        # output layer\n",
    "        t = np.matmul(v, self.W2)\n",
    "        self.inter.append(t)\n",
    "        \n",
    "        # softmax\n",
    "        y = np.exp(t)\n",
    "        y = y / np.sum(y, axis = 1, keepdims = True)\n",
    "        self.inter.append(y)\n",
    "        \n",
    "        self.pred = y\n",
    "        pass\n",
    "    \n",
    "    # prints the total loss\n",
    "    def printloss(self):\n",
    "        print 'The cross entropy loss is: %f' % self.crl\n",
    "        print 'The regularization loss is: %f' % self.regl\n",
    "        print 'The total loss is: %f' % self.totl\n",
    "        \n",
    "    # total loss\n",
    "    # beta is regularization coefficient\n",
    "    # outp is to set whether to print the loss\n",
    "    def loss(self, lab, beta = 1, outp = False):\n",
    "        \n",
    "        # cross entropy loss\n",
    "        u = np.zeros((lab.shape[0], self.op))\n",
    "        u[np.arange(lab.shape[0]), lab] = 1\n",
    "        self.exp = u\n",
    "        self.crl = - np.mean((u * np.log(self.pred)) + ((1 - u) * np.log(1 - self.pred)))\n",
    "        \n",
    "        # L2 regularization loss   \n",
    "        self.regl = 0.5 * beta * (np.mean(np.square(self.W1[:-1,:])) + np.sum(np.square(self.W2[:-1,:])))\n",
    "        \n",
    "        # total loss\n",
    "        self.totl = self.crl + self.regl\n",
    "        \n",
    "        # print if required\n",
    "        if outp:\n",
    "            print 'The cross entropy loss is: %f' % self.crl\n",
    "            print 'The regularization loss is: %f' % self.regl\n",
    "            print 'The total loss is: %f' % self.totl\n",
    "            \n",
    "        pass     \n",
    "        \n",
    "    # backward pass\n",
    "    # beta is regularization coefficient\n",
    "    # alpha is learning rate\n",
    "    # outp is to set whether to print the loss\n",
    "    def backward(self, lab, beta = 1, alpha = 0.1, outp = False):\n",
    "        \n",
    "        # loss caculation\n",
    "        self.loss (lab, beta, outp)\n",
    "        \n",
    "        self.der = []\n",
    "        # backprop over cross entropy loss\n",
    "        self.der.append((self.pred - self.exp) / (self.pred * (1 - self.pred)))\n",
    "        \n",
    "        # backprop over softmax\n",
    "        self.der.append (self.pred * (self.der[0] - np.sum(self.pred * self.der[0], axis = 1, keepdims = True)))\n",
    "        \n",
    "        # backprop over output layer\n",
    "        # weights derivative\n",
    "        self.der.append (np.matmul(self.inter[3].T, self.der[1]))\n",
    "        # input derivative\n",
    "        self.der.append (np.matmul(self.der[1], self.W2.T))\n",
    "        self.der.append (self.der[3][:,:-1])\n",
    "        \n",
    "        # backprop over hidden layer\n",
    "        # weights derivative\n",
    "        self.der.append (np.matmul(self.inter[1].T, self.der[4] * (self.inter[2] > 0)))\n",
    "        \n",
    "        # L2 derivatives        \n",
    "        self.der.append (beta * np.append(self.W2[:-1,:], np.zeros((1, self.op)), axis = 0))\n",
    "        self.der.append (beta * np.append(self.W1[:-1,:], np.zeros((1, self.hn)), axis = 0))\n",
    "        \n",
    "        # final total derivatives\n",
    "        self.W1der = self.der[5] + self.der[7]\n",
    "        self.W2der = self.der[2] + self.der[6]\n",
    "        \n",
    "        # changing weights accordingly\n",
    "        self.W1 = self.W1 - alpha * self.W1der\n",
    "        self.W2 = self.W2 - alpha * self.W2der\n",
    "        del self.der\n",
    "        pass\n",
    "    \n",
    "    # prints the classification accuracy\n",
    "    def classacc (self, lab):\n",
    "        count = 0\n",
    "        h = (np.argmax(self.pred, axis = 1) == lab)\n",
    "        for x in h:\n",
    "            if x:\n",
    "                count += 1\n",
    "        print 'The classification accuracy is: %d / %d = %f %%' %(count, h.shape[0], (count/h.shape[0])*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performs mini-batch SGD\n",
    "# hl: hidden layer size\n",
    "# alpha: learning rate\n",
    "# beta: regularization parameter\n",
    "# bsize: batch size for SGD\n",
    "# epoch: number of iterations for training\n",
    "def minisgd (hlsize = 50, alpha = 10**-5, beta = 10, bsize = 10, epoch = 5000):\n",
    "    \n",
    "    dl = DataLoader()\n",
    "    img, lab = dl.load_data()\n",
    "    u = NN(hlsize, 784, 10)\n",
    "    \n",
    "    crl_history = []\n",
    "    regl_history = []\n",
    "    totl_history = []\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        ti, tl = dl.create_batches(img, lab, bsize)\n",
    "        u.forward(ti)\n",
    "        u.backward(tl, beta, alpha)\n",
    "        crl_history.append(u.crl)\n",
    "        regl_history.append(u.regl)\n",
    "        totl_history.append(u.totl)\n",
    "    crl_history.append(u.crl)\n",
    "    regl_history.append(u.regl)\n",
    "    totl_history.append(u.totl)\n",
    "    \n",
    "    print 'Initially'\n",
    "    print 'The cross entropy loss was: %f' % crl_history[0]\n",
    "    print 'The regularization loss was: %f' % regl_history[0]\n",
    "    print 'The total loss was: %f' % totl_history[0]\n",
    "    print '\\nAfter %d passes' % (i+1)\n",
    "    print 'The cross entropy loss is: %f' % crl_history[-1]\n",
    "    print 'The regularization loss is: %f' % regl_history[-1]\n",
    "    print 'The total loss is: %f' % totl_history[-1]\n",
    "    \n",
    "    print '\\nOn the train set:'\n",
    "    u.forward(img)\n",
    "    u.classacc(lab)\n",
    "    \n",
    "    plt.plot(crl_history, 'b-')\n",
    "    plt.plot(regl_history, 'g-')\n",
    "    plt.plot(totl_history, 'r-')\n",
    "    plt.show()\n",
    "    \n",
    "    imgte, labte = dl.load_data(mode = 'test')\n",
    "    u.forward(imgte)\n",
    "    print '\\nOn the test set:'\n",
    "    u.loss(labte, beta, True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1, ..., 8, 1, 5], dtype=uint8)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8105\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "h = (np.argmax(u.pred, axis = 1) == labte)\n",
    "for x in h:\n",
    "    if x:\n",
    "        count += 1\n",
    "print count\n",
    "print h.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
